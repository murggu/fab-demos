{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "19b25a84-a630-470a-9b52-a546214a1b86",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Fact - Sale\n",
                "\n",
                "This cell reads raw data from the _Files_ section of the lakehouse, adds additional columns for different date parts and the same information is being used to create partitioned fact delta table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "23da9c7a-a5ef-413c-98c0-38d9d344f958",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, year, month, quarter\n",
                "\n",
                "table_name = 'fact_sale'\n",
                "\n",
                "df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/parquet/full/fact_sale_1y_full')\n",
                "df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\n",
                "df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\n",
                "df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n",
                "\n",
                "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d140d7c6-c0c2-45b5-8038-9d062747e957",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Dimensions\n",
                "This cell creates a function to read raw data from the _Files_ section of the lakehouse for the table name passed as a parameter. Next, it creates a list of dimension tables. Finally, it has a _for loop_ to loop through the list of tables and call above function with each table name as parameter to read data for that specific table and create delta table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "e52fe7a3-01e5-4cfa-ac58-b7cb17b7153d",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.types import *\n",
                "\n",
                "def loadFullDataFromSource(table_name):\n",
                "    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/parquet/full/' + table_name)\n",
                "    df = df.select([c for c in df.columns if c != 'Photo'])\n",
                "    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n",
                "\n",
                "full_tables = [\n",
                "    'dimension_city',\n",
                "    'dimension_customer',\n",
                "    'dimension_date',\n",
                "    'dimension_employee',\n",
                "    'dimension_stock_item'\n",
                "    ]\n",
                "\n",
                "for table in full_tables:\n",
                "    loadFullDataFromSource(table)"
            ]
        }
    ],
    "metadata": {
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "2c52a91e-6ef4-4364-a525-33ceae21618a",
                "default_lakehouse_name": "wwilakehouse",
                "default_lakehouse_workspace_id": "953110e1-d237-4866-9e9a-f4278a2eefcd",
                "known_lakehouses": [
                    {
                        "id": "82e78e41-11a3-448e-a1aa-0bc48fc09cb6"
                    },
                    {
                        "id": "2c52a91e-6ef4-4364-a525-33ceae21618a"
                    }
                ]
            }
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "notebook_environment": {},
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {},
                "enableDebugMode": false
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "widgets": {}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
