{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Approach #1 - sale_by_date_city\n",
                "In this cell, you are creating three different Spark dataframes, each referencing an existing delta table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T23:14:44.0653167Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T23:14:57.8304026Z\",\"execution_finish_time\":\"2023-04-14T23:15:57.0058225Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "df_fact_sale = spark.read.table(\"wwilakehouse.fact_sale\") \n",
                "df_dimension_date = spark.read.table(\"wwilakehouse.dimension_date\")\n",
                "df_dimension_city = spark.read.table(\"wwilakehouse.dimension_city\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "In this cell, you are joining these tables using the dataframes created earlier, doing group by to generate aggregation, renaming few of the columns and finally writing it as delta table in the _Tables_ section of the lakehouse."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T23:14:44.0704646Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T23:15:57.4087615Z\",\"execution_finish_time\":\"2023-04-14T23:16:16.4931219Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "sale_by_date_city = df_fact_sale.alias(\"sale\") \\\n",
                ".join(df_dimension_date.alias(\"date\"), df_fact_sale.InvoiceDateKey == df_dimension_date.Date, \"inner\") \\\n",
                ".join(df_dimension_city.alias(\"city\"), df_fact_sale.CityKey == df_dimension_city.CityKey, \"inner\") \\\n",
                ".select(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\", \"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\\\n",
                ".groupBy(\"date.Date\", \"date.CalendarMonthLabel\", \"date.Day\", \"date.ShortMonth\", \"date.CalendarYear\", \"city.City\", \"city.StateProvince\", \"city.SalesTerritory\")\\\n",
                ".sum(\"sale.TotalExcludingTax\", \"sale.TaxAmount\", \"sale.TotalIncludingTax\", \"sale.Profit\")\\\n",
                ".withColumnRenamed(\"sum(TotalExcludingTax)\", \"SumOfTotalExcludingTax\")\\\n",
                ".withColumnRenamed(\"sum(TaxAmount)\", \"SumOfTaxAmount\")\\\n",
                ".withColumnRenamed(\"sum(TotalIncludingTax)\", \"SumOfTotalIncludingTax\")\\\n",
                ".withColumnRenamed(\"sum(Profit)\", \"SumOfProfit\")\\\n",
                ".orderBy(\"date.Date\", \"city.StateProvince\", \"city.City\")\n",
                "\n",
                "sale_by_date_city.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_city\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Approach #2 - sale_by_date_employee\n",
                "In this cell, you are creating a temporary Spark view by joining 3 tables, doing group by to generate aggregation, renaming few of the columns. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T23:14:44.0716871Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T23:16:16.8047041Z\",\"execution_finish_time\":\"2023-04-14T23:16:26.9405173Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "collapsed": false,
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "sparksql"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "CREATE OR REPLACE TEMPORARY VIEW sale_by_date_employee\n",
                "AS\n",
                "SELECT\n",
                "\tDD.Date, DD.CalendarMonthLabel\n",
                "    , DD.Day, DD.ShortMonth Month, CalendarYear Year\n",
                "\t,DE.PreferredName, DE.Employee\n",
                "\t,SUM(FS.TotalExcludingTax) SumOfTotalExcludingTax\n",
                "\t,SUM(FS.TaxAmount) SumOfTaxAmount\n",
                "\t,SUM(FS.TotalIncludingTax) SumOfTotalIncludingTax\n",
                "\t,SUM(Profit) SumOfProfit \n",
                "FROM wwilakehouse.fact_sale FS\n",
                "INNER JOIN wwilakehouse.dimension_date DD ON FS.InvoiceDateKey = DD.Date\n",
                "INNER JOIN wwilakehouse.dimension_Employee DE ON FS.SalespersonKey = DE.EmployeeKey\n",
                "GROUP BY DD.Date, DD.CalendarMonthLabel, DD.Day, DD.ShortMonth, DD.CalendarYear, DE.PreferredName, DE.Employee\n",
                "ORDER BY DD.Date ASC, DE.PreferredName ASC, DE.Employee ASC"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "In this cell, you are reading from the temporary Spark view created in the previous cell and and finally writing it as delta table in the _Tables_ section of the lakehouse."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T23:14:44.0729484Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T23:16:27.2707122Z\",\"execution_finish_time\":\"2023-04-14T23:16:46.344428Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "sale_by_date_employee = spark.sql(\"SELECT * FROM sale_by_date_employee\")\n",
                "sale_by_date_employee.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(\"Tables/aggregate_sale_by_date_employee\")"
            ]
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "language": "Python",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "notebook_environment": {},
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {},
                "enableDebugMode": false
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "trident": {
            "lakehouse": {}
        },
        "widgets": {}
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
